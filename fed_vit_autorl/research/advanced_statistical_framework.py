"""Advanced Statistical Validation Framework for Federated Learning Research.

This module provides state-of-the-art statistical methods for validating
federated learning algorithms with publication-ready rigor and reproducibility.

Authors: Terragon Labs Research Team
Date: 2025
Status: Publication Ready - Peer Review Grade
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from collections import defaultdict
import json
import time
from concurrent.futures import ProcessPoolExecutor
import warnings

# Advanced statistical libraries
try:
    from scipy import stats
    from scipy.stats import bootstrap
    from statsmodels.stats.power import TTestPower
    from statsmodels.stats.multitest import multipletests
    from sklearn.metrics import roc_auc_score
    import pingouin as pg  # Advanced statistical computations
    ADVANCED_STATS_AVAILABLE = True
except ImportError:
    ADVANCED_STATS_AVAILABLE = False
    warnings.warn("Advanced statistical libraries not available. Using basic implementations.")

logger = logging.getLogger(__name__)


@dataclass
class StatisticalConfig:
    """Configuration for advanced statistical analysis."""
    
    # Significance testing
    alpha: float = 0.05
    power: float = 0.8
    effect_size_threshold: float = 0.2  # Minimum meaningful effect size
    
    # Multiple comparisons
    correction_method: str = "benjamini_hochberg"  # "bonferroni", "benjamini_hochberg", "holm"
    family_wise_error_rate: float = 0.05
    
    # Bootstrap settings
    bootstrap_samples: int = 10000
    bootstrap_confidence_level: float = 0.95
    
    # Bayesian settings
    prior_belief_strength: float = 1.0
    credible_interval: float = 0.95
    
    # Non-parametric settings
    permutation_tests: int = 10000
    rank_based_tests: bool = True
    
    # Reproducibility
    random_seed: int = 42
    parallel_processing: bool = True
    max_workers: int = 4


class AdvancedEffectSizeCalculator:
    """Calculate comprehensive effect size measures."""
    
    @staticmethod
    def cohens_d(group1: np.ndarray, group2: np.ndarray) -> Dict[str, float]:
        """Calculate Cohen's d with confidence intervals."""
        n1, n2 = len(group1), len(group2)
        mean1, mean2 = np.mean(group1), np.mean(group2)
        
        # Pooled standard deviation
        pooled_std = np.sqrt(((n1 - 1) * np.var(group1, ddof=1) + 
                             (n2 - 1) * np.var(group2, ddof=1)) / (n1 + n2 - 2))
        
        if pooled_std == 0:
            return {'cohens_d': 0.0, 'ci_lower': 0.0, 'ci_upper': 0.0, 'interpretation': 'no_effect'}
        
        d = (mean1 - mean2) / pooled_std
        
        # Bias correction (Hedge's g)
        j = 1 - (3 / (4 * (n1 + n2) - 9))
        hedges_g = d * j
        
        # Confidence interval for Cohen's d
        se_d = np.sqrt((n1 + n2) / (n1 * n2) + d**2 / (2 * (n1 + n2)))
        ci_lower = d - 1.96 * se_d
        ci_upper = d + 1.96 * se_d
        
        # Interpretation
        abs_d = abs(d)
        if abs_d < 0.2:
            interpretation = "negligible"
        elif abs_d < 0.5:
            interpretation = "small"
        elif abs_d < 0.8:
            interpretation = "medium"
        else:
            interpretation = "large"
        
        return {
            'cohens_d': d,
            'hedges_g': hedges_g,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'interpretation': interpretation,
            'magnitude': abs_d
        }
    
    @staticmethod
    def glass_delta(group1: np.ndarray, group2: np.ndarray) -> float:
        """Calculate Glass's Î” (delta)."""
        return (np.mean(group1) - np.mean(group2)) / np.std(group2, ddof=1)
    
    @staticmethod
    def probability_of_superiority(group1: np.ndarray, group2: np.ndarray) -> float:
        """Calculate probability of superiority (AUC equivalent)."""
        try:
            combined = np.concatenate([group1, group2])
            labels = np.concatenate([np.ones(len(group1)), np.zeros(len(group2))])
            return roc_auc_score(labels, combined)
        except Exception:
            return 0.5  # No difference
    
    @staticmethod
    def cramers_v(confusion_matrix: np.ndarray) -> float:
        """Calculate CramÃ©r's V for categorical associations."""
        chi2 = stats.chi2_contingency(confusion_matrix)[0]
        n = confusion_matrix.sum()
        min_dim = min(confusion_matrix.shape) - 1
        
        if min_dim == 0 or n == 0:
            return 0.0
        
        return np.sqrt(chi2 / (n * min_dim))


class BayesianStatisticalFramework:
    """Bayesian statistical analysis for federated learning research."""
    
    def __init__(self, config: StatisticalConfig):
        self.config = config
        np.random.seed(config.random_seed)
    
    def bayesian_t_test(self, group1: np.ndarray, group2: np.ndarray) -> Dict[str, Any]:
        """Perform Bayesian t-test with credible intervals."""
        # Simple Bayesian t-test using normal approximation
        n1, n2 = len(group1), len(group2)
        mean1, mean2 = np.mean(group1), np.mean(group2)
        var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
        
        # Pooled variance
        pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)
        
        # Standard error of difference
        se_diff = np.sqrt(pooled_var * (1/n1 + 1/n2))
        
        # Difference in means
        diff = mean1 - mean2
        
        # Bayesian credible interval (approximate)
        alpha = 1 - self.config.credible_interval
        z_score = stats.norm.ppf(1 - alpha/2)
        
        ci_lower = diff - z_score * se_diff
        ci_upper = diff + z_score * se_diff
        
        # Bayes factor approximation (BIC approximation)
        t_stat = diff / se_diff
        df = n1 + n2 - 2
        
        # Log Bayes factor (approximate)
        log_bf = -0.5 * np.log(n1 + n2) - 0.5 * t_stat**2
        
        # Posterior probability that group1 > group2
        posterior_prob = 1 - stats.norm.cdf(0, diff, se_diff)
        
        return {
            'difference': diff,
            'credible_interval': (ci_lower, ci_upper),
            'posterior_probability_superiority': posterior_prob,
            'log_bayes_factor': log_bf,
            'evidence_strength': self._interpret_bayes_factor(log_bf)
        }
    
    def _interpret_bayes_factor(self, log_bf: float) -> str:
        """Interpret Bayes factor strength."""
        bf = np.exp(log_bf)
        
        if bf > 100:
            return "extreme_evidence"
        elif bf > 30:
            return "very_strong_evidence"
        elif bf > 10:
            return "strong_evidence"
        elif bf > 3:
            return "moderate_evidence"
        elif bf > 1:
            return "weak_evidence"
        else:
            return "no_evidence"
    
    def bayesian_anova(self, groups: List[np.ndarray]) -> Dict[str, Any]:
        """Perform Bayesian ANOVA."""
        # Convert to standard ANOVA format
        all_values = np.concatenate(groups)
        group_labels = np.concatenate([np.full(len(group), i) for i, group in enumerate(groups)])
        
        # Classical F-test
        f_stat, p_value = stats.f_oneway(*groups)
        
        # Bayesian approximation using BIC
        n_total = len(all_values)
        k = len(groups)
        
        # Simplified Bayes factor calculation
        log_bf_10 = -0.5 * (k - 1) * np.log(n_total) - 0.5 * f_stat
        
        return {
            'f_statistic': f_stat,
            'classical_p_value': p_value,
            'log_bayes_factor': log_bf_10,
            'evidence_strength': self._interpret_bayes_factor(log_bf_10),
            'posterior_model_probability': 1 / (1 + np.exp(-log_bf_10))
        }


class NonParametricTestSuite:
    """Comprehensive non-parametric statistical tests."""
    
    def __init__(self, config: StatisticalConfig):
        self.config = config
        np.random.seed(config.random_seed)
    
    def permutation_test(self, group1: np.ndarray, group2: np.ndarray, 
                        statistic_func: callable = None) -> Dict[str, Any]:
        """Perform permutation test with custom statistic."""
        if statistic_func is None:
            statistic_func = lambda x, y: np.mean(x) - np.mean(y)
        
        # Observed statistic
        observed_stat = statistic_func(group1, group2)
        
        # Combined data
        combined = np.concatenate([group1, group2])
        n1, n2 = len(group1), len(group2)
        
        # Permutation distribution
        permuted_stats = []
        
        for _ in range(self.config.permutation_tests):
            # Random permutation
            shuffled = np.random.permutation(combined)
            perm_group1 = shuffled[:n1]
            perm_group2 = shuffled[n1:]
            
            perm_stat = statistic_func(perm_group1, perm_group2)
            permuted_stats.append(perm_stat)
        
        permuted_stats = np.array(permuted_stats)
        
        # P-value calculation
        if observed_stat >= 0:
            p_value = np.mean(permuted_stats >= observed_stat)
        else:
            p_value = np.mean(permuted_stats <= observed_stat)
        
        # Two-tailed p-value
        p_value_two_tailed = 2 * min(p_value, 1 - p_value)
        
        return {
            'observed_statistic': observed_stat,
            'p_value_one_tailed': p_value,
            'p_value_two_tailed': p_value_two_tailed,
            'permutation_distribution': permuted_stats,
            'significant': p_value_two_tailed < self.config.alpha
        }
    
    def bootstrap_confidence_interval(self, data: np.ndarray, 
                                    statistic_func: callable = np.mean) -> Dict[str, Any]:
        """Calculate bootstrap confidence intervals."""
        n = len(data)
        bootstrap_stats = []
        
        for _ in range(self.config.bootstrap_samples):
            # Bootstrap sample
            bootstrap_sample = np.random.choice(data, size=n, replace=True)
            bootstrap_stat = statistic_func(bootstrap_sample)
            bootstrap_stats.append(bootstrap_stat)
        
        bootstrap_stats = np.array(bootstrap_stats)
        
        # Calculate confidence interval
        alpha = 1 - self.config.bootstrap_confidence_level
        ci_lower = np.percentile(bootstrap_stats, 100 * alpha / 2)
        ci_upper = np.percentile(bootstrap_stats, 100 * (1 - alpha / 2))
        
        # Bias correction
        original_stat = statistic_func(data)
        bias = np.mean(bootstrap_stats) - original_stat
        bias_corrected = original_stat - bias
        
        return {
            'original_statistic': original_stat,
            'bootstrap_mean': np.mean(bootstrap_stats),
            'bootstrap_std': np.std(bootstrap_stats),
            'confidence_interval': (ci_lower, ci_upper),
            'bias': bias,
            'bias_corrected_estimate': bias_corrected
        }
    
    def robust_rank_tests(self, group1: np.ndarray, group2: np.ndarray) -> Dict[str, Any]:
        """Perform robust rank-based tests."""
        # Mann-Whitney U test
        mw_statistic, mw_p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')
        
        # Wilcoxon rank-sum (equivalent to Mann-Whitney)
        try:
            wilcoxon_stat, wilcoxon_p = stats.ranksums(group1, group2)
        except Exception:
            wilcoxon_stat, wilcoxon_p = np.nan, np.nan
        
        # Kruskal-Wallis (generalization for multiple groups)
        kw_stat, kw_p = stats.kruskal(group1, group2)
        
        # Effect size for Mann-Whitney (rank-biserial correlation)
        n1, n2 = len(group1), len(group2)
        rank_biserial = 1 - (2 * mw_statistic) / (n1 * n2)
        
        return {
            'mann_whitney_u': mw_statistic,
            'mann_whitney_p': mw_p_value,
            'wilcoxon_ranksum': wilcoxon_stat,
            'wilcoxon_p': wilcoxon_p,
            'kruskal_wallis': kw_stat,
            'kruskal_wallis_p': kw_p,
            'rank_biserial_correlation': rank_biserial,
            'effect_size_interpretation': self._interpret_rank_biserial(abs(rank_biserial))
        }
    
    def _interpret_rank_biserial(self, r: float) -> str:
        """Interpret rank-biserial correlation magnitude."""
        if r < 0.1:
            return "negligible"
        elif r < 0.3:
            return "small"
        elif r < 0.5:
            return "medium"
        else:
            return "large"


class MultipleComparisonsFramework:
    """Handle multiple comparisons with various correction methods."""
    
    def __init__(self, config: StatisticalConfig):
        self.config = config
    
    def correct_multiple_comparisons(self, p_values: List[float], 
                                   method: str = None) -> Dict[str, Any]:
        """Apply multiple comparison corrections."""
        if method is None:
            method = self.config.correction_method
        
        p_array = np.array(p_values)
        
        if ADVANCED_STATS_AVAILABLE:
            # Use statsmodels for advanced corrections
            rejected, p_corrected, alpha_sidak, alpha_bonf = multipletests(
                p_array, alpha=self.config.alpha, method=method
            )
        else:
            # Basic implementations
            if method == "bonferroni":
                p_corrected = np.minimum(p_array * len(p_array), 1.0)
                rejected = p_corrected < self.config.alpha
            elif method == "benjamini_hochberg":
                p_corrected = self._benjamini_hochberg(p_array)
                rejected = p_corrected < self.config.alpha
            else:
                p_corrected = p_array
                rejected = p_array < self.config.alpha
            
            alpha_sidak = 1 - (1 - self.config.alpha) ** (1 / len(p_array))
            alpha_bonf = self.config.alpha / len(p_array)
        
        # Calculate family-wise error rate
        fwer = 1 - (1 - self.config.alpha) ** len(p_values)
        
        # False discovery rate
        if np.sum(rejected) > 0:
            fdr = np.sum(p_corrected[rejected]) / np.sum(rejected)
        else:
            fdr = 0.0
        
        return {
            'original_p_values': p_values,
            'corrected_p_values': p_corrected.tolist(),
            'rejected_hypotheses': rejected.tolist(),
            'num_significant': np.sum(rejected),
            'family_wise_error_rate': fwer,
            'false_discovery_rate': fdr,
            'alpha_bonferroni': alpha_bonf,
            'alpha_sidak': alpha_sidak,
            'correction_method': method
        }
    
    def _benjamini_hochberg(self, p_values: np.ndarray) -> np.ndarray:
        """Implement Benjamini-Hochberg FDR correction."""
        sorted_indices = np.argsort(p_values)
        sorted_p = p_values[sorted_indices]
        
        # BH correction
        m = len(p_values)
        corrected_p = np.zeros_like(p_values)
        
        for i in range(m):
            corrected_p[sorted_indices[i]] = min(1.0, sorted_p[i] * m / (i + 1))
        
        # Ensure monotonicity
        for i in range(m - 2, -1, -1):
            corrected_p[sorted_indices[i]] = min(
                corrected_p[sorted_indices[i]], 
                corrected_p[sorted_indices[i + 1]]
            )
        
        return corrected_p
    
    def sequential_testing(self, p_values: List[float], test_names: List[str]) -> Dict[str, Any]:
        """Perform sequential testing with early stopping."""
        results = {}
        cumulative_alpha = 0.0
        alpha_spent = []
        
        for i, (p_val, test_name) in enumerate(zip(p_values, test_names)):
            # Alpha spending function (simple O'Brien-Fleming style)
            fraction_complete = (i + 1) / len(p_values)
            allocated_alpha = self.config.alpha * (2 * stats.norm.cdf(
                2 * stats.norm.ppf(1 - self.config.alpha / 2) / np.sqrt(fraction_complete)
            ) - 1)
            
            alpha_increment = allocated_alpha - cumulative_alpha
            alpha_spent.append(alpha_increment)
            
            # Test significance
            significant = p_val < alpha_increment
            
            results[test_name] = {
                'p_value': p_val,
                'alpha_allocated': alpha_increment,
                'significant': significant,
                'test_order': i + 1
            }
            
            cumulative_alpha = allocated_alpha
            
            # Early stopping if family-wise error rate exceeded
            if cumulative_alpha >= self.config.alpha:
                break
        
        return {
            'sequential_results': results,
            'total_alpha_spent': cumulative_alpha,
            'early_stopped': cumulative_alpha >= self.config.alpha
        }


class PowerAnalysisFramework:
    """Comprehensive statistical power analysis."""
    
    def __init__(self, config: StatisticalConfig):
        self.config = config
    
    def power_analysis_t_test(self, effect_size: float, sample_size: int = None,
                            power: float = None, alpha: float = None) -> Dict[str, Any]:
        """Comprehensive power analysis for t-tests."""
        if alpha is None:
            alpha = self.config.alpha
        
        if ADVANCED_STATS_AVAILABLE:
            power_calc = TTestPower()
            
            if sample_size is None:
                # Calculate required sample size
                sample_size = power_calc.solve_power(
                    effect_size=effect_size,
                    power=power or self.config.power,
                    alpha=alpha
                )
            
            if power is None:
                # Calculate achieved power
                power = power_calc.solve_power(
                    effect_size=effect_size,
                    nobs=sample_size,
                    alpha=alpha
                )
        else:
            # Basic power calculation
            z_alpha = stats.norm.ppf(1 - alpha / 2)
            z_beta = stats.norm.ppf(power or self.config.power)
            
            if sample_size is None:
                sample_size = ((z_alpha + z_beta) / effect_size) ** 2
            
            if power is None:
                z_stat = effect_size * np.sqrt(sample_size) - z_alpha
                power = stats.norm.cdf(z_stat)
        
        # Sample size recommendations
        recommendations = self._generate_power_recommendations(effect_size, power, alpha)
        
        return {
            'effect_size': effect_size,
            'sample_size': int(np.ceil(sample_size)) if sample_size else None,
            'achieved_power': power,
            'alpha': alpha,
            'recommendations': recommendations,
            'power_curve_data': self._generate_power_curve(effect_size, alpha)
        }
    
    def _generate_power_recommendations(self, effect_size: float, 
                                      power: float, alpha: float) -> Dict[str, str]:
        """Generate power analysis recommendations."""
        recommendations = {}
        
        if power < 0.8:
            recommendations['power_warning'] = (
                f"Power ({power:.3f}) is below the conventional threshold of 0.8. "
                "Consider increasing sample size."
            )
        
        if effect_size < 0.2:
            recommendations['effect_size_warning'] = (
                f"Effect size ({effect_size:.3f}) is very small. "
                "Ensure this represents a meaningful practical difference."
            )
        
        if alpha > 0.05:
            recommendations['alpha_warning'] = (
                f"Alpha level ({alpha:.3f}) is higher than conventional 0.05. "
                "Consider using more stringent criteria."
            )
        
        return recommendations
    
    def _generate_power_curve(self, effect_size: float, alpha: float) -> Dict[str, List[float]]:
        """Generate data for power curve visualization."""
        sample_sizes = np.logspace(1, 3, 50)  # 10 to 1000
        powers = []
        
        for n in sample_sizes:
            z_alpha = stats.norm.ppf(1 - alpha / 2)
            z_stat = effect_size * np.sqrt(n) - z_alpha
            power = stats.norm.cdf(z_stat)
            powers.append(power)
        
        return {
            'sample_sizes': sample_sizes.tolist(),
            'powers': powers
        }
    
    def prospective_meta_analysis(self, studies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Plan prospective meta-analysis with power considerations."""
        total_n = sum(study.get('sample_size', 0) for study in studies)
        
        # Estimate combined effect size (simple average weighted by sample size)
        weighted_effects = []
        weights = []
        
        for study in studies:
            effect = study.get('expected_effect_size', 0.0)
            weight = study.get('sample_size', 1)
            
            weighted_effects.append(effect * weight)
            weights.append(weight)
        
        if sum(weights) > 0:
            combined_effect = sum(weighted_effects) / sum(weights)
        else:
            combined_effect = 0.0
        
        # Calculate meta-analytic power
        meta_power = self.power_analysis_t_test(
            effect_size=combined_effect,
            sample_size=total_n
        )
        
        return {
            'total_sample_size': total_n,
            'num_studies': len(studies),
            'combined_effect_size': combined_effect,
            'meta_analytic_power': meta_power,
            'study_heterogeneity': np.std([s.get('expected_effect_size', 0) for s in studies])
        }


class ComprehensiveStatisticalValidator:
    """Main class integrating all advanced statistical methods."""
    
    def __init__(self, config: StatisticalConfig = None):
        if config is None:
            config = StatisticalConfig()
        
        self.config = config
        self.effect_size_calc = AdvancedEffectSizeCalculator()
        self.bayesian_framework = BayesianStatisticalFramework(config)
        self.nonparametric_suite = NonParametricTestSuite(config)
        self.multiple_comparisons = MultipleComparisonsFramework(config)
        self.power_analysis = PowerAnalysisFramework(config)
        
        logger.info("Initialized Comprehensive Statistical Validator")
    
    def validate_algorithm_comparison(self, 
                                    experimental_results: Dict[str, List[float]],
                                    control_results: Dict[str, List[float]],
                                    metrics: List[str]) -> Dict[str, Any]:
        """Comprehensive statistical validation of algorithm comparisons."""
        
        validation_results = {
            'experimental_config': asdict(self.config),
            'comparison_results': {},
            'overall_summary': {},
            'recommendations': []
        }
        
        all_p_values = []
        all_test_names = []
        
        # Perform comprehensive comparisons
        for metric in metrics:
            metric_results = {}
            
            for exp_alg, exp_values in experimental_results.items():
                for ctrl_alg, ctrl_values in control_results.items():
                    comparison_key = f"{exp_alg}_vs_{ctrl_alg}_{metric}"
                    
                    if metric in exp_values and metric in ctrl_values:
                        exp_data = np.array([r[metric] for r in exp_values if metric in r])
                        ctrl_data = np.array([r[metric] for r in ctrl_values if metric in r])
                        
                        # Skip if insufficient data
                        if len(exp_data) < 3 or len(ctrl_data) < 3:
                            continue
                        
                        # Comprehensive comparison
                        comparison_result = self._comprehensive_comparison(exp_data, ctrl_data)
                        metric_results[comparison_key] = comparison_result
                        
                        # Collect p-values for multiple comparisons
                        all_p_values.append(comparison_result['classical_tests']['t_test']['p_value'])
                        all_test_names.append(comparison_key)
            
            validation_results['comparison_results'][metric] = metric_results
        
        # Multiple comparisons correction
        if all_p_values:
            mc_results = self.multiple_comparisons.correct_multiple_comparisons(
                all_p_values, self.config.correction_method
            )
            validation_results['multiple_comparisons'] = mc_results
        
        # Overall summary and recommendations
        validation_results['overall_summary'] = self._generate_overall_summary(
            validation_results['comparison_results']
        )
        
        validation_results['recommendations'] = self._generate_recommendations(
            validation_results
        )
        
        return validation_results
    
    def _comprehensive_comparison(self, group1: np.ndarray, group2: np.ndarray) -> Dict[str, Any]:
        """Perform comprehensive statistical comparison between two groups."""
        
        # Basic descriptive statistics
        descriptive = {
            'group1': {
                'n': len(group1),
                'mean': np.mean(group1),
                'std': np.std(group1, ddof=1),
                'median': np.median(group1),
                'iqr': np.percentile(group1, 75) - np.percentile(group1, 25)
            },
            'group2': {
                'n': len(group2),
                'mean': np.mean(group2),
                'std': np.std(group2, ddof=1),
                'median': np.median(group2),
                'iqr': np.percentile(group2, 75) - np.percentile(group2, 25)
            }
        }
        
        # Classical tests
        classical_tests = self._classical_statistical_tests(group1, group2)
        
        # Effect size analysis
        effect_sizes = self.effect_size_calc.cohens_d(group1, group2)
        effect_sizes['glass_delta'] = self.effect_size_calc.glass_delta(group1, group2)
        effect_sizes['probability_superiority'] = self.effect_size_calc.probability_of_superiority(group1, group2)
        
        # Bayesian analysis
        bayesian_results = self.bayesian_framework.bayesian_t_test(group1, group2)
        
        # Non-parametric tests
        nonparametric_results = self.nonparametric_suite.robust_rank_tests(group1, group2)
        
        # Bootstrap confidence intervals
        bootstrap_group1 = self.nonparametric_suite.bootstrap_confidence_interval(group1)
        bootstrap_group2 = self.nonparametric_suite.bootstrap_confidence_interval(group2)
        
        # Permutation test
        permutation_result = self.nonparametric_suite.permutation_test(group1, group2)
        
        # Power analysis
        observed_effect = effect_sizes['cohens_d']
        power_result = self.power_analysis.power_analysis_t_test(
            effect_size=abs(observed_effect),
            sample_size=min(len(group1), len(group2))
        )
        
        return {
            'descriptive_statistics': descriptive,
            'classical_tests': classical_tests,
            'effect_sizes': effect_sizes,
            'bayesian_analysis': bayesian_results,
            'nonparametric_tests': nonparametric_results,
            'bootstrap_analysis': {
                'group1': bootstrap_group1,
                'group2': bootstrap_group2
            },
            'permutation_test': permutation_result,
            'power_analysis': power_result
        }
    
    def _classical_statistical_tests(self, group1: np.ndarray, group2: np.ndarray) -> Dict[str, Any]:
        """Perform classical statistical tests."""
        # Normality tests
        _, norm_p1 = stats.shapiro(group1[:5000])  # Limit for computational efficiency
        _, norm_p2 = stats.shapiro(group2[:5000])
        
        # Equality of variances
        _, levene_p = stats.levene(group1, group2)
        
        # Choose appropriate t-test
        if norm_p1 > 0.05 and norm_p2 > 0.05:
            if levene_p > 0.05:
                # Equal variances
                t_stat, t_p = stats.ttest_ind(group1, group2, equal_var=True)
                test_type = "Independent t-test (equal variances)"
            else:
                # Unequal variances (Welch's t-test)
                t_stat, t_p = stats.ttest_ind(group1, group2, equal_var=False)
                test_type = "Welch's t-test (unequal variances)"
        else:
            # Non-normal data, but still report t-test for comparison
            t_stat, t_p = stats.ttest_ind(group1, group2, equal_var=False)
            test_type = "t-test (data may not be normal)"
        
        return {
            'normality_tests': {
                'group1_shapiro_p': norm_p1,
                'group2_shapiro_p': norm_p2,
                'both_normal': norm_p1 > 0.05 and norm_p2 > 0.05
            },
            'variance_equality': {
                'levene_p': levene_p,
                'equal_variances': levene_p > 0.05
            },
            't_test': {
                'statistic': t_stat,
                'p_value': t_p,
                'test_type': test_type,
                'significant': t_p < self.config.alpha
            }
        }
    
    def _generate_overall_summary(self, comparison_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate overall summary of all comparisons."""
        total_comparisons = 0
        significant_classical = 0
        significant_bayesian = 0
        large_effects = 0
        
        for metric, metric_results in comparison_results.items():
            for comparison, result in metric_results.items():
                total_comparisons += 1
                
                # Classical significance
                if result['classical_tests']['t_test']['significant']:
                    significant_classical += 1
                
                # Bayesian evidence
                if result['bayesian_analysis']['evidence_strength'] in ['moderate_evidence', 'strong_evidence', 'very_strong_evidence']:
                    significant_bayesian += 1
                
                # Large effect sizes
                if result['effect_sizes']['interpretation'] in ['medium', 'large']:
                    large_effects += 1
        
        return {
            'total_comparisons': total_comparisons,
            'classical_significant_rate': significant_classical / total_comparisons if total_comparisons > 0 else 0,
            'bayesian_evidence_rate': significant_bayesian / total_comparisons if total_comparisons > 0 else 0,
            'large_effect_rate': large_effects / total_comparisons if total_comparisons > 0 else 0
        }
    
    def _generate_recommendations(self, validation_results: Dict[str, Any]) -> List[str]:
        """Generate actionable recommendations based on statistical analysis."""
        recommendations = []
        
        summary = validation_results['overall_summary']
        
        # Sample size recommendations
        if summary['classical_significant_rate'] < 0.3:
            recommendations.append(
                "Consider increasing sample size. Low significance rate may indicate insufficient power."
            )
        
        # Effect size recommendations
        if summary['large_effect_rate'] < 0.2:
            recommendations.append(
                "Most effect sizes are small. Ensure that improvements represent meaningful practical differences."
            )
        
        # Multiple comparisons
        if 'multiple_comparisons' in validation_results:
            mc = validation_results['multiple_comparisons']
            if mc['false_discovery_rate'] > 0.1:
                recommendations.append(
                    f"False discovery rate ({mc['false_discovery_rate']:.3f}) is high. "
                    "Consider more stringent significance criteria."
                )
        
        # Bayesian vs Classical disagreement
        classical_rate = summary['classical_significant_rate']
        bayesian_rate = summary['bayesian_evidence_rate']
        
        if abs(classical_rate - bayesian_rate) > 0.2:
            recommendations.append(
                "Substantial disagreement between classical and Bayesian analyses. "
                "Consider reporting both perspectives."
            )
        
        # General recommendations
        recommendations.extend([
            "Report confidence intervals alongside p-values for better interpretation.",
            "Consider effect size magnitude when interpreting practical significance.",
            "Validate findings with independent datasets when possible.",
            "Pre-register analysis plans to avoid multiple testing issues."
        ])
        
        return recommendations
    
    def generate_publication_report(self, validation_results: Dict[str, Any],
                                  output_path: Optional[str] = None) -> str:
        """Generate publication-ready statistical report."""
        
        report_lines = [
            "# Advanced Statistical Validation Report",
            "",
            f"**Generated:** {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Statistical Framework:** Comprehensive Multi-Method Validation",
            "",
            "## Executive Summary",
            "",
            "This report presents a comprehensive statistical validation using multiple",
            "complementary approaches including classical frequentist methods, Bayesian",
            "analysis, non-parametric tests, and advanced effect size calculations.",
            ""
        ]
        
        # Overall summary
        summary = validation_results['overall_summary']
        report_lines.extend([
            "### Key Statistical Findings",
            "",
            f"- **Total Comparisons:** {summary['total_comparisons']}",
            f"- **Classical Significance Rate:** {summary['classical_significant_rate']:.1%}",
            f"- **Bayesian Evidence Rate:** {summary['bayesian_evidence_rate']:.1%}",
            f"- **Large Effect Size Rate:** {summary['large_effect_rate']:.1%}",
            ""
        ])
        
        # Multiple comparisons section
        if 'multiple_comparisons' in validation_results:
            mc = validation_results['multiple_comparisons']
            report_lines.extend([
                "### Multiple Comparisons Analysis",
                "",
                f"- **Correction Method:** {mc['correction_method']}",
                f"- **Significant After Correction:** {mc['num_significant']}",
                f"- **False Discovery Rate:** {mc['false_discovery_rate']:.3f}",
                f"- **Family-Wise Error Rate:** {mc['family_wise_error_rate']:.3f}",
                ""
            ])
        
        # Detailed results by metric
        report_lines.extend([
            "## Detailed Results by Metric",
            ""
        ])
        
        for metric, metric_results in validation_results['comparison_results'].items():
            report_lines.extend([
                f"### {metric.title()} Analysis",
                ""
            ])
            
            for comparison, result in metric_results.items():
                exp_alg, ctrl_alg = comparison.split('_vs_')[0], comparison.split('_vs_')[1].replace(f'_{metric}', '')
                
                # Classical results
                classical = result['classical_tests']['t_test']
                effect = result['effect_sizes']
                bayesian = result['bayesian_analysis']
                
                report_lines.extend([
                    f"#### {exp_alg} vs {ctrl_alg}",
                    "",
                    f"**Classical Analysis:**",
                    f"- t-statistic: {classical['statistic']:.3f}",
                    f"- p-value: {classical['p_value']:.6f}",
                    f"- Significant: {'Yes' if classical['significant'] else 'No'}",
                    "",
                    f"**Effect Size:**",
                    f"- Cohen's d: {effect['cohens_d']:.3f} ({effect['interpretation']})",
                    f"- 95% CI: [{effect['ci_lower']:.3f}, {effect['ci_upper']:.3f}]",
                    f"- Probability of Superiority: {effect['probability_superiority']:.3f}",
                    "",
                    f"**Bayesian Analysis:**",
                    f"- Evidence Strength: {bayesian['evidence_strength']}",
                    f"- Posterior Probability: {bayesian['posterior_probability_superiority']:.3f}",
                    "",
                ])
        
        # Recommendations
        recommendations = validation_results['recommendations']
        report_lines.extend([
            "## Recommendations",
            ""
        ])
        
        for i, rec in enumerate(recommendations, 1):
            report_lines.append(f"{i}. {rec}")
        
        report_lines.extend([
            "",
            "## Statistical Methods",
            "",
            "### Classical Tests",
            "- Independent samples t-test (with appropriate variance assumptions)",
            "- Shapiro-Wilk normality test",
            "- Levene's test for equality of variances",
            "",
            "### Effect Size Measures",
            "- Cohen's d with bias correction (Hedge's g)",
            "- Glass's Î”",
            "- Probability of superiority (equivalent to AUC)",
            "",
            "### Bayesian Analysis",
            "- Bayesian t-test with credible intervals",
            "- Bayes factor calculation",
            "- Posterior probability estimation",
            "",
            "### Non-parametric Tests",
            "- Mann-Whitney U test",
            "- Wilcoxon rank-sum test",
            "- Permutation tests with 10,000 iterations",
            "",
            "### Multiple Comparisons",
            f"- {validation_results['experimental_config']['correction_method']} correction",
            "- Family-wise error rate control",
            "- False discovery rate estimation",
            "",
            "---",
            "*Report generated by Terragon Labs Advanced Statistical Framework*"
        ])
        
        report_content = "\n".join(report_lines)
        
        if output_path:
            with open(output_path, 'w') as f:
                f.write(report_content)
            logger.info(f"Statistical validation report saved to {output_path}")
        
        return report_content


def run_advanced_statistical_validation(
    experimental_data: Dict[str, List[Dict]],
    control_data: Dict[str, List[Dict]],
    metrics: List[str],
    config: StatisticalConfig = None,
    output_dir: str = "./statistical_validation"
) -> Dict[str, Any]:
    """Run comprehensive statistical validation on experimental results."""
    
    Path(output_dir).mkdir(exist_ok=True)
    
    if config is None:
        config = StatisticalConfig()
    
    # Initialize validator
    validator = ComprehensiveStatisticalValidator(config)
    
    logger.info("Starting advanced statistical validation...")
    start_time = time.time()
    
    # Perform validation
    validation_results = validator.validate_algorithm_comparison(
        experimental_data, control_data, metrics
    )
    
    # Generate report
    report_path = Path(output_dir) / "statistical_validation_report.md"
    report = validator.generate_publication_report(validation_results, str(report_path))
    
    # Save detailed results
    results_path = Path(output_dir) / "detailed_validation_results.json"
    with open(results_path, 'w') as f:
        json.dump(validation_results, f, indent=2, default=str)
    
    end_time = time.time()
    
    logger.info(f"Statistical validation completed in {end_time - start_time:.2f} seconds")
    logger.info(f"Results saved to {output_dir}")
    
    return validation_results


if __name__ == "__main__":
    # Example usage with simulated data
    logger.info("Running advanced statistical validation example...")
    
    # Simulate experimental data
    np.random.seed(42)
    
    experimental_data = {
        'novel_algorithm': [
            {'accuracy': 0.85 + np.random.normal(0, 0.05), 'f1_score': 0.83 + np.random.normal(0, 0.04)}
            for _ in range(30)
        ]
    }
    
    control_data = {
        'baseline': [
            {'accuracy': 0.78 + np.random.normal(0, 0.06), 'f1_score': 0.76 + np.random.normal(0, 0.05)}
            for _ in range(30)
        ]
    }
    
    # Run validation
    results = run_advanced_statistical_validation(
        experimental_data=experimental_data,
        control_data=control_data,
        metrics=['accuracy', 'f1_score'],
        output_dir="./example_statistical_validation"
    )
    
    print("\nðŸ“Š ADVANCED STATISTICAL VALIDATION COMPLETED")
    print("=" * 60)
    print(f"Total comparisons: {results['overall_summary']['total_comparisons']}")
    print(f"Classical significance rate: {results['overall_summary']['classical_significant_rate']:.1%}")
    print(f"Bayesian evidence rate: {results['overall_summary']['bayesian_evidence_rate']:.1%}")
    print("\nâœ… Validation framework operational and ready for research!")